{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8884,"status":"ok","timestamp":1676974859380,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"gZbDkxx2r2GK"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Anaconda\\envs\\TransNet\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"FkXNJVYcsVxQ"},"source":["# 数据生成"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":907,"status":"ok","timestamp":1676974860276,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"4JopaG-DsexR"},"outputs":[],"source":["import torch\n","import h5py\n","import numpy as np\n","\n","class DataGenerator:\n","  def __init__(self, Nt, Nr, c, c_sub, number, max_value, data_root=None):\n","    self.Nt = Nt\n","    self.Nr = Nr\n","    self.c = c\n","    self.c_sub = c_sub\n","    self.number = number\n","    self.max_value = max_value\n","    self.data_root = data_root\n","    \n","  def generate(self):\n","    raw_data = self.max_value*np.random.uniform(-1, 1, (self.number, self.c_sub*self.c, self.Nt*self.Nr))\n","    return raw_data\n","\n","  def fromDataRoot(self):\n","    mat_transpose = h5py.File(self.data_root)['Final'][:self.c_sub*self.number]\n","    mat = mat_transpose.T.reshape(self.Nt, self.Nr, self.c, self.c_sub, self.number)\n","    mat = mat.transpose(-1, 0, 1, 2, 3)\n","    \n","    mat_flat = mat.flatten()\n","    mat_real = np.array([mat_flat[i][0] for i in range(mat_flat.shape[0])])\n","    mat_img = np.array([mat_flat[i][1] for i in range(mat_flat.shape[0])])\n","    \n","    mat_real.resize((self.number, self.Nt, self.Nr, self.c, self.c_sub))\n","    mat_img.resize((self.number, self.Nt, self.Nr, self.c, self.c_sub))\n","    \n","    return mat_real"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":479,"status":"ok","timestamp":1676974860751,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"RWfW_jrIvz8-"},"outputs":[],"source":["# raw data example\n","g = DataGenerator(16, 16, 5, 512, 100, 0.001)\n","raw_data = g.generate()"]},{"cell_type":"markdown","metadata":{"id":"vL1RqQLO_GiN"},"source":["# 数据集和数据加载器"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676974861162,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"PaygfvXgDVUV"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class MimoDataset(Dataset):\n","  def __init__(self, raw_data_array):\n","    self.raw_data_array = torch.tensor(raw_data_array, dtype=torch.float32)\n","\n","  def __len__(self):\n","    return self.raw_data_array.shape[0]\n","  \n","  def __getitem__(self, ind):\n","    src, target = self.raw_data_array[ind], self.raw_data_array.clone()[ind]\n","    return src, target"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1676974861162,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"jO4sANn5GVSl"},"outputs":[],"source":["mimo_data = MimoDataset(raw_data_array=raw_data)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676974861163,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"GSVjMrDCEUAK"},"outputs":[],"source":["from torch.utils.data import random_split\n","from torch.utils.data import DataLoader\n","\n","class MimoDataLoader:\n","  def __init__(self, dataset, batch_size=16, train_rho=0.8, val_rho=0.1):\n","    self.dataset = dataset\n","    self.dataset_len = len(self.dataset)\n","    self.batch_size = batch_size\n","    self.train_num = int(train_rho*self.dataset_len)\n","    self.val_num = int(val_rho*self.dataset_len)\n","    self.test_num = self.dataset_len - self.train_num - self.val_num\n","  \n","  def getDataLoader(self):\n","    train_dataset, val_dataset, test_dataset, = random_split(self.dataset, [self.train_num, self.val_num, self.test_num])\n","    \n","    train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n","    test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=True)\n","    \n","    return train_dataloader, test_dataloader, val_dataloader"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676974861163,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"ihgICxZcPmLI"},"outputs":[],"source":["m = MimoDataLoader(mimo_data)\n","train_dataloader, test_dataloader, val_dataloader = m.getDataLoader()"]},{"cell_type":"markdown","metadata":{"id":"u5SjzAsd0NrM"},"source":["# 模型定义"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1676974864477,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"kUqtGR9wwrZj"},"outputs":[],"source":["\n","# 范式 1\n","class FrequenceNet(nn.Module):\n","  def __init__(self, extract_model, restore_model, input_dim, compress_ratio):\n","    super().__init__()\n","    self.extract_model = extract_model\n","    self.restore_model = restore_model\n","    assert not input_dim%compress_ratio, ValueError('compress_ratio must be divisible by input_dim.')\n","    self.compress_dim = int(input_dim//compress_ratio)\n","    self.full_connect_encoder = nn.Linear(input_dim, self.compress_dim)\n","    self.full_connect_decoder = nn.Linear(self.compress_dim, input_dim)\n","    \n","    for m in self.modules():\n","      if isinstance(m, nn.Linear):\n","        nn.init.xavier_normal_(m.weight)\n","        nn.init.constant_(m.bias, 0)     \n","\n","  def forward(self, data):\n","    extract_feature = self.extract_model(data)\n","    compress_feature = self.full_connect_encoder(extract_feature)\n","    restore_feature_raw = self.full_connect_decoder(compress_feature)\n","    restore_feature = self.restore_model(restore_feature_raw)\n","    return restore_feature"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# 范式 2\n","class ExtractionStrategies:\n","    def __init__(self, input_dim, compress_ratio) -> None:\n","        self.input_dim = input_dim\n","        self.compress_ratio = compress_ratio\n","        self.compress_dim = int(input_dim//compress_ratio)\n","        \n","    def equidistantExtraction(self):\n","        return lambda x: x[:, : :self.compress_ratio, :].clone()\n","    \n","\n","class ChoseNet(nn.Module):\n","    def __init__(self, strategy_f, restore_model, input_num, restore_num):\n","        super().__init__()\n","        self.strategy_f = strategy_f\n","        self.restore_model = restore_model\n","        self.full_connect_decoder = nn.Linear(input_num, restore_num)\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_normal_(m.weight)\n","                nn.init.constant_(m.bias, 0)     \n","        \n","    def forward(self, data):\n","        data = self.strategy_f(data)\n","        restore_feature_raw = self.full_connect_decoder(data.transpose(-1, -2)).transpose(-1, -2)\n","        restore_feature = self.restore_model(restore_feature_raw)\n","        return restore_feature"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Get restore model\n","ES = ExtractionStrategies(512, 4)\n","restore_model = nn.Linear(512, 512)\n","chose_model = ChoseNet(ES.equidistantExtraction(), restore_model=restore_model, input_num= int(512*5/4), restore_num=2560)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1676974864477,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"0Au8sJskP81b"},"outputs":[],"source":["# Get frequence_model\n","input_dim = 512\n","extract_model = nn.Linear(input_dim, input_dim)\n","frequence_model = FrequenceNet(extract_model, extract_model, input_dim, compress_ratio=4)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1676974864477,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"b2SB1jMMQwT1","outputId":"1248ff35-539e-438e-d34b-5a83bae1923f"},"outputs":[{"name":"stdout","output_type":"stream","text":["extract_model.weight\n","extract_model.bias\n","full_connect_encoder.weight\n","full_connect_encoder.bias\n","full_connect_decoder.weight\n","full_connect_decoder.bias\n","\n","********************************************************************************\n","\n","restore_model.weight\n","restore_model.bias\n","full_connect_decoder.weight\n","full_connect_decoder.bias\n"]}],"source":["# Check params should be updated during training\n","for name, param in frequence_model.named_parameters():\n","    if param.requires_grad:\n","        print(name)\n","print()\n","print('*'*80)\n","print()\n","for name, param in chose_model.named_parameters():\n","    if param.requires_grad:\n","        print(name)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["restore_model.weight Parameter containing:\n","tensor([[ 0.0111,  0.0230,  0.0243,  ...,  0.0272,  0.1175, -0.0493],\n","        [-0.0325, -0.0670, -0.0608,  ..., -0.0586,  0.0368,  0.0015],\n","        [ 0.0793,  0.0475, -0.0697,  ...,  0.0247, -0.0256,  0.0415],\n","        ...,\n","        [ 0.0003,  0.0112, -0.0159,  ...,  0.0244,  0.0035,  0.0395],\n","        [ 0.0259, -0.0424,  0.0493,  ...,  0.0829,  0.0299,  0.0089],\n","        [-0.0220, -0.0447, -0.0053,  ..., -0.0040,  0.0327,  0.0020]],\n","       requires_grad=True)\n","restore_model.bias Parameter containing:\n","tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n","full_connect_decoder.weight Parameter containing:\n","tensor([[-0.0618,  0.0205, -0.0114,  ..., -0.0210, -0.0210,  0.0884],\n","        [ 0.0074, -0.0110,  0.0136,  ..., -0.0001,  0.0115,  0.0285],\n","        [-0.0248, -0.0213, -0.0028,  ...,  0.0050, -0.0345,  0.0236],\n","        ...,\n","        [-0.0175, -0.0010, -0.0042,  ...,  0.0290,  0.0367, -0.0338],\n","        [-0.0101, -0.0036,  0.0135,  ..., -0.0048, -0.0199,  0.0600],\n","        [ 0.0062,  0.0123, -0.0009,  ...,  0.0207, -0.0160,  0.0477]],\n","       requires_grad=True)\n","full_connect_decoder.bias Parameter containing:\n","tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)\n"]}],"source":["# See the parameters\n","for name, param in chose_model.named_parameters():\n","    print(name, param)\n","\n","# for name, param in frequence_model.named_parameters():\n","#     print(name, param)"]},{"cell_type":"markdown","metadata":{"id":"SEdQ2fTJ-3jn"},"source":["# 训练器"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1202,"status":"ok","timestamp":1676974865661,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"-xrDqr3Q5KKm"},"outputs":[],"source":["import torch\n","import numpy as np\n","import math\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class Trainer:\n","    def __init__(\n","        self,\n","        model,\n","        epoch,\n","        train_dataloader,\n","        val_dataloader,\n","        criterion,\n","        optimizer,\n","        lr_scheduler,\n","        res_root='./res',\n","        checkpoint_path='./res/checkpoint/',\n","        loss_path='./res/loss/',\n","        check_frequence=0,\n","        training_step=1,\n","        valid_step=1,\n","        \n","    ):\n","        self.model = model\n","        self.model.to(device)\n","        self.epoch = epoch\n","        self.train_dataloader = train_dataloader\n","        self.val_dataloader = val_dataloader\n","        self.training_step = training_step\n","        self.valid_step = valid_step\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.lr_scheduler = lr_scheduler\n","        self.check_frequence = check_frequence\n","        self.checkpoint_path = checkpoint_path\n","        self.log = lambda x: 10*math.log10(x)\n","        self.loss = {\n","            'train_mse':[],\n","            'val_mse':[],\n","        }\n","        self.res_root = res_root\n","        self.loss_path = loss_path\n","    \n","    def train(self):\n","        for e in range(self.epoch):\n","            self.train_epoch()\n","            self.valid_epoch()\n","            print('Epoch:{:n}, Average training loss: {:3f}, Average valid loss: {:3f} \\n'.format(\n","                int(e + 1),\n","                self.loss['train_mse'][-1], \n","                self.loss['val_mse'][-1]\n","                )\n","            )\n","            \n","            self.lr_scheduler.step()\n","        \n","    def train_epoch(self):\n","        self.model.train()\n","        running_loss = []\n","        \n","        for ind, data in enumerate(self.train_dataloader):\n","            src, tag = data[0].to(device), data[1].to(device)\n","            \n","            self.optimizer.zero_grad()\n","            out = self.model(src)\n","            loss = self.criterion(out, tag, reduction='sum')\n","            loss.backward()\n","            \n","            train_batch_mse = self.log(loss.item()/src.shape[0])\n","            running_loss.append(train_batch_mse)\n","            self.optimizer.step()\n","\n","            if ind == self.training_step:\n","                break\n","        \n","        epoch_loss = np.mean(running_loss)\n","        \n","        self.loss['train_mse'].append(epoch_loss)\n","\n","    def valid_epoch(self):\n","        self.model.eval()\n","        running_loss = []\n","        \n","        with torch.no_grad():\n","            for ind, data in enumerate(self.val_dataloader):\n","                src, tag = data[0].to(device), data[1].to(device)\n","                out = self.model(src)\n","                loss = self.criterion(out, tag, reduction='sum')\n","                running_loss.append(self.log(loss.item()/src.shape[0]))\n","\n","                if ind == self.valid_step:\n","                    break\n","        \n","        epoch_loss = np.mean(running_loss)     \n","        self.loss['val_mse'].append(epoch_loss)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676974865661,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"7mFCz2pL93x-","outputId":"3d22110c-6532-436f-c8b8-ca73f4980e92"},"outputs":[],"source":["from torch.nn.functional import mse_loss\n","from torch.optim.lr_scheduler import LambdaLR\n","import torch.optim as optim\n","\n","criterion = mse_loss\n","max_epoch = 100"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["optimizer_1 = optim.Adam(params=chose_model.parameters(), lr=1e-2)\n","scheduler_1 = LambdaLR(\n","    optimizer=optimizer_1,\n","    lr_lambda= lambda x: (max_epoch - x)/max_epoch,\n","    verbose = False\n",")\n","\n","optimizer_2 = optim.Adam(params=frequence_model.parameters(), lr=1e-2)\n","scheduler_2 = LambdaLR(\n","    optimizer=optimizer_2,\n","    lr_lambda= lambda x: (max_epoch - x)/max_epoch,\n","    verbose = False\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["trainer1 = Trainer(\n","    model=chose_model,\n","    epoch=100,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    training_step=1,\n","    valid_step=1,\n","    criterion=criterion,\n","    optimizer=optimizer_1,\n","    lr_scheduler=scheduler_1,\n","    check_frequence=5,\n","    checkpoint_path='.',\n","    loss_path='.')\n","\n","trainer2 = Trainer(\n","    model=frequence_model,\n","    epoch=100,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    training_step=1,\n","    valid_step=1,\n","    criterion=criterion,\n","    optimizer=optimizer_2,\n","    lr_scheduler=scheduler_2,\n","    check_frequence=5,\n","    checkpoint_path='.',\n","    loss_path='.')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419960,"status":"ok","timestamp":1676975285619,"user":{"displayName":"崔耀东","userId":"15262221923760036897"},"user_tz":-480},"id":"eR9aTMKZYwq_","outputId":"b2b0c0a2-f7de-40c6-fccf-7e305e1ce32a"},"outputs":[{"ename":"RuntimeError","evalue":"CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer1\u001b[39m.\u001b[39;49mtrain()\n","Cell \u001b[1;32mIn [14], line 47\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch):\n\u001b[1;32m---> 47\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch()\n\u001b[0;32m     48\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalid_epoch()\n\u001b[0;32m     49\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m{:n}\u001b[39;00m\u001b[39m, Average training loss: \u001b[39m\u001b[39m{:3f}\u001b[39;00m\u001b[39m, Average valid loss: \u001b[39m\u001b[39m{:3f}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     50\u001b[0m             \u001b[39mint\u001b[39m(e \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[0;32m     51\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss[\u001b[39m'\u001b[39m\u001b[39mtrain_mse\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \n\u001b[0;32m     52\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss[\u001b[39m'\u001b[39m\u001b[39mval_mse\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     53\u001b[0m             )\n\u001b[0;32m     54\u001b[0m         )\n","Cell \u001b[1;32mIn [14], line 66\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m src, tag \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 66\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(src)\n\u001b[0;32m     67\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(out, tag, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32md:\\Anaconda\\envs\\TransNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn [9], line 26\u001b[0m, in \u001b[0;36mChoseNet.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     24\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy_f(data)\n\u001b[0;32m     25\u001b[0m restore_feature_raw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_connect_decoder(data\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m restore_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_model(restore_feature_raw)\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m restore_feature\n","File \u001b[1;32md:\\Anaconda\\envs\\TransNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Anaconda\\envs\\TransNet\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[1;32md:\\Anaconda\\envs\\TransNet\\lib\\site-packages\\torch\\nn\\functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight):\n\u001b[0;32m   1846\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[1;32m-> 1847\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"]}],"source":["trainer1.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch:1, Average training loss: 7.992342, Average valid loss: 16.217878 \n","\n","Epoch:2, Average training loss: 16.153667, Average valid loss: 20.800424 \n","\n","Epoch:3, Average training loss: 22.739703, Average valid loss: 28.191642 \n","\n","Epoch:4, Average training loss: 24.027857, Average valid loss: 22.136681 \n","\n","Epoch:5, Average training loss: 20.003230, Average valid loss: 22.925304 \n","\n","Epoch:6, Average training loss: 17.853768, Average valid loss: 22.381251 \n","\n","Epoch:7, Average training loss: 18.757588, Average valid loss: 16.361514 \n","\n","Epoch:8, Average training loss: 17.824463, Average valid loss: 14.942323 \n","\n","Epoch:9, Average training loss: 12.872041, Average valid loss: 15.134530 \n","\n","Epoch:10, Average training loss: 15.009959, Average valid loss: 10.707622 \n","\n","Epoch:11, Average training loss: 9.959765, Average valid loss: 11.807482 \n","\n","Epoch:12, Average training loss: 11.835893, Average valid loss: 10.111846 \n","\n","Epoch:13, Average training loss: 9.547379, Average valid loss: 9.340590 \n","\n","Epoch:14, Average training loss: 9.410215, Average valid loss: 8.859071 \n","\n","Epoch:15, Average training loss: 8.176033, Average valid loss: 5.989020 \n","\n","Epoch:16, Average training loss: 5.885481, Average valid loss: 6.074297 \n","\n","Epoch:17, Average training loss: 5.995415, Average valid loss: 5.711765 \n","\n","Epoch:18, Average training loss: 5.268349, Average valid loss: 3.072638 \n","\n","Epoch:19, Average training loss: 2.913044, Average valid loss: 3.389854 \n","\n","Epoch:20, Average training loss: 3.501888, Average valid loss: 3.270813 \n","\n","Epoch:21, Average training loss: 2.370132, Average valid loss: -0.386928 \n","\n","Epoch:22, Average training loss: -0.073120, Average valid loss: 0.697251 \n","\n","Epoch:23, Average training loss: 0.666009, Average valid loss: -0.227875 \n","\n","Epoch:24, Average training loss: -0.854301, Average valid loss: -1.226453 \n","\n","Epoch:25, Average training loss: -1.220078, Average valid loss: -1.687565 \n","\n","Epoch:26, Average training loss: -1.831356, Average valid loss: -2.621671 \n","\n","Epoch:27, Average training loss: -2.686314, Average valid loss: -3.236970 \n","\n","Epoch:28, Average training loss: -3.815761, Average valid loss: -4.439542 \n","\n","Epoch:29, Average training loss: -4.346054, Average valid loss: -4.029422 \n","\n","Epoch:30, Average training loss: -4.161381, Average valid loss: -5.361383 \n","\n","Epoch:31, Average training loss: -5.509805, Average valid loss: -5.557757 \n","\n","Epoch:32, Average training loss: -5.548983, Average valid loss: -5.700798 \n","\n","Epoch:33, Average training loss: -6.057522, Average valid loss: -6.555945 \n","\n","Epoch:34, Average training loss: -6.512843, Average valid loss: -6.676855 \n","\n","Epoch:35, Average training loss: -6.763210, Average valid loss: -7.221305 \n","\n","Epoch:36, Average training loss: -7.222944, Average valid loss: -7.101301 \n","\n","Epoch:37, Average training loss: -7.206237, Average valid loss: -7.484767 \n","\n","Epoch:38, Average training loss: -7.600453, Average valid loss: -7.748084 \n","\n","Epoch:39, Average training loss: -7.718258, Average valid loss: -7.873423 \n","\n","Epoch:40, Average training loss: -7.929333, Average valid loss: -8.025382 \n","\n","Epoch:41, Average training loss: -8.024038, Average valid loss: -7.992108 \n","\n","Epoch:42, Average training loss: -8.064568, Average valid loss: -8.286345 \n","\n","Epoch:43, Average training loss: -8.305212, Average valid loss: -8.312453 \n","\n","Epoch:44, Average training loss: -8.294404, Average valid loss: -8.304399 \n","\n","Epoch:45, Average training loss: -8.338474, Average valid loss: -8.366016 \n","\n","Epoch:46, Average training loss: -8.387406, Average valid loss: -8.426083 \n","\n","Epoch:47, Average training loss: -8.444324, Average valid loss: -8.501500 \n","\n","Epoch:48, Average training loss: -8.493876, Average valid loss: -8.478796 \n","\n","Epoch:49, Average training loss: -8.496760, Average valid loss: -8.531070 \n","\n","Epoch:50, Average training loss: -8.548281, Average valid loss: -8.560545 \n","\n","Epoch:51, Average training loss: -8.562994, Average valid loss: -8.554293 \n","\n","Epoch:52, Average training loss: -8.558980, Average valid loss: -8.576101 \n","\n","Epoch:53, Average training loss: -8.580025, Average valid loss: -8.580047 \n","\n","Epoch:54, Average training loss: -8.593427, Average valid loss: -8.603207 \n","\n","Epoch:55, Average training loss: -8.607277, Average valid loss: -8.608057 \n","\n","Epoch:56, Average training loss: -8.609850, Average valid loss: -8.606167 \n","\n","Epoch:57, Average training loss: -8.612156, Average valid loss: -8.623149 \n","\n","Epoch:58, Average training loss: -8.623223, Average valid loss: -8.625140 \n","\n","Epoch:59, Average training loss: -8.628582, Average valid loss: -8.626082 \n","\n","Epoch:60, Average training loss: -8.633249, Average valid loss: -8.633901 \n","\n","Epoch:61, Average training loss: -8.636087, Average valid loss: -8.636030 \n","\n","Epoch:62, Average training loss: -8.640132, Average valid loss: -8.639745 \n","\n","Epoch:63, Average training loss: -8.645338, Average valid loss: -8.643003 \n","\n","Epoch:64, Average training loss: -8.646371, Average valid loss: -8.643778 \n","\n","Epoch:65, Average training loss: -8.649628, Average valid loss: -8.647086 \n","\n","Epoch:66, Average training loss: -8.651228, Average valid loss: -8.649791 \n","\n","Epoch:67, Average training loss: -8.650665, Average valid loss: -8.651599 \n","\n","Epoch:68, Average training loss: -8.652762, Average valid loss: -8.652999 \n","\n","Epoch:69, Average training loss: -8.652994, Average valid loss: -8.654928 \n","\n","Epoch:70, Average training loss: -8.657204, Average valid loss: -8.656698 \n","\n","Epoch:71, Average training loss: -8.659236, Average valid loss: -8.658296 \n","\n","Epoch:72, Average training loss: -8.662196, Average valid loss: -8.659741 \n","\n","Epoch:73, Average training loss: -8.659015, Average valid loss: -8.661193 \n","\n","Epoch:74, Average training loss: -8.666575, Average valid loss: -8.662465 \n","\n","Epoch:75, Average training loss: -8.664201, Average valid loss: -8.663405 \n","\n","Epoch:76, Average training loss: -8.667027, Average valid loss: -8.664829 \n","\n","Epoch:77, Average training loss: -8.663014, Average valid loss: -8.666043 \n","\n","Epoch:78, Average training loss: -8.669455, Average valid loss: -8.666769 \n","\n","Epoch:79, Average training loss: -8.672298, Average valid loss: -8.668202 \n","\n","Epoch:80, Average training loss: -8.669765, Average valid loss: -8.669352 \n","\n","Epoch:81, Average training loss: -8.672465, Average valid loss: -8.670218 \n","\n","Epoch:82, Average training loss: -8.671188, Average valid loss: -8.670790 \n","\n","Epoch:83, Average training loss: -8.672695, Average valid loss: -8.671716 \n","\n","Epoch:84, Average training loss: -8.671271, Average valid loss: -8.672827 \n","\n","Epoch:85, Average training loss: -8.673243, Average valid loss: -8.673619 \n","\n","Epoch:86, Average training loss: -8.673946, Average valid loss: -8.674252 \n","\n","Epoch:87, Average training loss: -8.677443, Average valid loss: -8.674906 \n","\n","Epoch:88, Average training loss: -8.674152, Average valid loss: -8.675670 \n","\n","Epoch:89, Average training loss: -8.679106, Average valid loss: -8.676258 \n","\n","Epoch:90, Average training loss: -8.681708, Average valid loss: -8.676662 \n","\n","Epoch:91, Average training loss: -8.677694, Average valid loss: -8.677105 \n","\n","Epoch:92, Average training loss: -8.682036, Average valid loss: -8.677697 \n","\n","Epoch:93, Average training loss: -8.683203, Average valid loss: -8.678145 \n","\n","Epoch:94, Average training loss: -8.677854, Average valid loss: -8.678493 \n","\n","Epoch:95, Average training loss: -8.681429, Average valid loss: -8.678737 \n","\n","Epoch:96, Average training loss: -8.682340, Average valid loss: -8.678852 \n","\n","Epoch:97, Average training loss: -8.684082, Average valid loss: -8.679025 \n","\n","Epoch:98, Average training loss: -8.680908, Average valid loss: -8.679230 \n","\n","Epoch:99, Average training loss: -8.679468, Average valid loss: -8.679388 \n","\n","Epoch:100, Average training loss: -8.686386, Average valid loss: -8.679469 \n","\n"]}],"source":["trainer2.train()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 测试"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["FrequenceNet(\n","  (extract_model): Linear(in_features=256, out_features=256, bias=True)\n","  (restore_model): Linear(in_features=256, out_features=256, bias=True)\n","  (full_connect_encoder): Linear(in_features=256, out_features=64, bias=True)\n","  (full_connect_decoder): Linear(in_features=64, out_features=256, bias=True)\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["frequence_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["FrequenceNet(\n","  (extract_model): Linear(in_features=256, out_features=256, bias=True)\n","  (restore_model): Linear(in_features=256, out_features=256, bias=True)\n","  (full_connect_encoder): Linear(in_features=256, out_features=64, bias=True)\n","  (full_connect_decoder): Linear(in_features=64, out_features=256, bias=True)\n",")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["frequence_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["test_src, test_tag= next(iter(test_dataloader))\n","test_src = test_src.to(device)\n","test_tag = test_tag.to(device)\n","test_src.is_cuda"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_out = frequence_model(test_src)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def simple_norm(tensor):\n","    max_value, min_value = torch.max(tensor), torch.min(tensor)\n","    norm_F = lambda x: (x - min_value)/abs(max_value)\n","    tensor.apply_(norm_F)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["norm_out = torch.nn.functional.normalize(model_out, p=2, dim=0)\n","norm_src = torch.nn.functional.normalize(test_src, p=2, dim=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.0020, device='cuda:0', grad_fn=<MaxBackward1>)\n"]}],"source":["print(torch.max(model_out))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([10, 1280, 256])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["test_src.shape"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyOx1GKEXP8lqdPHefUfXPFr","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"TransNet","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"c2cf101b24c22296489a0b9d8c96604021120cfdbf98246cab2f47583247d4b4"}}},"nbformat":4,"nbformat_minor":0}
